\documentclass[11pt]{amsart}
\usepackage{geometry}
\geometry{a4paper}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{bm}
\usepackage{verbatim}
\usepackage{amsaddr} 
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{A brief guide to Principal Component Analysis as implemented in OpenPCA}

\author{Matteo Tommasini}
\address{
Dipartimento di Chimica, Materiali e Ingegneria Chimica "G. Natta"
Politecnico di Milano, Piazza Leonardo da Vinci 32 -- 20133 Milano (Italy); matteo.tommasini@polimi.it}

\date{\today}

\begin{document}

\maketitle

%
The Principal Component Analysis (PCA) was introduced in 1933 by Harold Hotelling \cite{Hotelling1933} in the context of psicometric data analysis. Since its birth, PCA has been widely applied to many fields where multivariate datasets have to be dealt with.
The easier approach to introduce PCA, by also taking into consideration its numerical implementation in Matlab, is through a fully algebraic approach that focuses on the variance-covariance matrix of the dataset and its spectral decomposition.
%
Let us introduce first the multivariate dataset matrix $\bm X_{ov}$, which along each row stores the results of one multivariate observation along a given number of variables ($N_v$). The adopted notation for the dataset matrix highlights the different role of row {\it vs.} column indexes. The different observations are identified in the $\bm X_{ov}$ matrix by the row index ($o$), whereas the different variables of each multivariate measurement (observation) are identified by the column index ($v$). In the context of spectroscopy, each row represents one spectrum, and the different variables are the wavenumbers at which the instrument has recorded a given spectral intensity ({\it e.g.}, Raman intensity, or absorbance). Hence, because of the adopted notation, we have the following identities:
%
\begin{eqnarray}
\bm X = \bm X_{ov}
\\ \nonumber
\bm X_{vo} = \left( \bm X_{ov} \right)^t = \bm X^t,
\end{eqnarray}
%
where $^t$ indicates matrix transposition. As described later, the variance-covariance matrix among the variables of the dataset can be straightforwardly introduced through the matrix of the centered dataset, $\bm \chi_{ov}$:
%
\begin{equation}
\label{eq.centering}
\bm \chi_{ov} = \bm X_{ov} -  \bm X_{\langle o \rangle v},
\end{equation}
%
where $\bm X_{\langle o \rangle v}$ represents the row vector of the average values of the variables over the number of $N_o$ observations, and its elements are given by:
%
\begin{equation}
\label{eq.average}
X_{\langle o \rangle v} = \frac{1}{N_o} \sum_{o} X_{ov} 
\end{equation}
%
We adopt in Eq. (\ref{eq.centering}) the same abuse of notation used in Matlab: by subtracting a row vector to a matrix actually one subtracts the given row vector to each row of the matrix. Hence Eq. (\ref{eq.centering}) is implemented in Matlab as simply as \mbox{chi = X - mean(X)}, because the Matlab function mean(X) gives the row vector corresponding to the average of all the rows of the X matrix -- which effectively corresponds to averaging out with respect to the available observations (see above).
%
The variance-covariance matrix among the variables of the dataset ($\bm \Sigma_{vv}$) is defined as follows:
%
\begin{eqnarray}
\label{eq.variance_covariance}
\Sigma_{v_1 v_2} = 
\frac{1}{N_o-1} 
\sum_{o} 
\left( X_{o v_1} - X_{\langle o \rangle v_1} \right) 
\, 
\left( X_{o v_2} - X_{\langle o \rangle v_2} \right) 
=
\\ \nonumber
=
\frac{1}{N_o-1} 
\sum_{o}
\chi_{o v_1} \chi_{o v_2}
\end{eqnarray}
%
The last equality shows that, with matrix notation, the variance covariance matrix is simply given by:
%
\begin{eqnarray}
\label{eq.variance_covariance2}
\bm \Sigma_{vv} = 
\frac{1}{N_o-1} 
({\bm \chi_{ov}})^t {\bm \chi_{ov}} = \frac{1}{N_o-1} 
{\bm \chi_{vo}} {\bm \chi_{ov}}.
\end{eqnarray}
%
Clearly, by definition, $\bm \Sigma$ is a symmetric matrix, and it is positive definite\footnote{This is straightforward to show by using the expression $\bm \Sigma = \bm A^t \bm A$, with $\bm A = \bm \chi / \sqrt{N_o-1}$. For $\bm \Sigma$ to be positive defined it should be $\bm x^t \bm \Sigma \bm x \ge 0$ for a generic $\bm x$ column vector.
%
By considering that $\bm \Sigma = \bm A^t \bm A$, such requirements becomes $0 \le \bm x^t \bm \Sigma \bm x = \bm x^t \bm A^t \bm A \bm x = (\bm A \bm x)^t (\bm A \bm x) = \left| \bm A \bm x \right|^2$. The last step clearly proves the statement, because it represents the squared modulus of the generic vector $\bm A \bm x$ (certainly a non-negative quantity).}. Therefore it admits spectral decomposition by the orthogonal matrix of its eigenvectors, and the eigenvalues are positive quantities \cite{Schott2016}.
%
The matrix eigenvalue problem of the variance-covariance matrix is written as:
%
\begin{eqnarray}
\label{eq.eigenproblem_sigma}
\bm \Sigma_{vv} \bm L_{vs} = \bm L_{vs} \bm \sigma_{ss} 
\end{eqnarray}
%
In Eq. (\ref{eq.eigenproblem_sigma}) $\bm \sigma_{ss}$ is the diagonal matrix of the eigenvalues of $\bm \Sigma_{vv}$ and $\bm L_{vs}$ is the orthogonal matrix of the eigenvectors of $\bm \Sigma_{vv}$. The orthogonality of $\bm L_{vs}$ implies:
%
\begin{eqnarray}
\bm L_{vs} \bm L_{sv}  = \bm 1_{vv}
\\ \nonumber
\bm L_{sv} \bm L_{vs}  = \bm 1_{ss}. 
\end{eqnarray}
%
Therefore, by left-multiplying Eq. (\ref{eq.eigenproblem_sigma}) by $\bm L_{sv}$, and by considering its orthonormality, one obtains the spectral decomposition of the variance-covariance matrix:
%
\begin{eqnarray}
\label{eq.spectral-decomposition}
\bm L_{sv} \bm \Sigma_{vv} \bm L_{vs}
=
\bm \sigma_{ss}
\end{eqnarray}
%
By sustituting in the right-hand side of Eq. (\ref{eq.spectral-decomposition}) the definition of $\bm \Sigma_{vv} = \bm \chi_{vo} \bm \chi_{ov} / (N_o-1)$ (cfr. Eq. \ref{eq.variance_covariance2}), one obtains:
%
\begin{eqnarray}
\label{eq.decomposed}
\bm \sigma_{ss} =
\frac{1}{N_o - 1} \; \bm L_{sv} \bm \chi_{vo} \; \bm \chi_{ov} \bm L_{vs} 
\end{eqnarray}
%
Similarly to the definition of a variance-covariance matrix (Eq. (\ref{eq.variance_covariance})), it is then possible to identify in the right-hand side of Eq. (\ref{eq.decomposed}) a structure given by the product of a matrix (defined $\bm S$) by its transpose:
%
\begin{eqnarray}
\bm \sigma_{ss}
=
\left[ \frac{1}{\sqrt{N_o - 1}} \bm L_{sv} \bm \chi_{vo} \right]
\left[ \bm \chi_{ov} \bm L_{vs} \frac{1}{\sqrt{N_o - 1}} \right] 
=
\bm S_{so} \bm S_{os}
=
\bm S^t \bm S.
\end{eqnarray}
%
The rows of such a matrix ($\bm S_{os}$) -- named the {\em scores} matrix -- define the observations ($o$ label) through the so-called principal components ($s$ label):
%
\begin{eqnarray}
\label{eq.scores}
\bm S_{os} \equiv \left[ \frac{1}{\sqrt{N_o - 1}} \bm \chi_{ov} \bm L_{vs} \right]
\end{eqnarray}
%
The matrix of the eigenvectors of the variance-covariance matrix ($\bm L_{vs}$), which is named the {\em loadings} matrix, defines the linear relationship existing between each principal component and the set of variables.
%
The PCA scatterplot {\em e.g.} of the first two principal components (PC1, PC2) can be obtained by plotting on the Cartesian $xy$ plane the first column of the $\bm S$ matrix ($x$ coordinates) vs. the second column of the $\bm S$ matrix ($y$ coordinates). This plot immediately allows judging data clustering or the presence of outliers. Such scatterplots can be of course extended to other principal components ({\em i.e.}, to other columns of the $\bm S$ matrix).
Sometimes, the scores matrix is normalized in such a way to produce an associated variance-covariance matrix (over the $s$ variables) that is a unit matrix. This normalization is simply done as follows:
%
\begin{eqnarray}
\bm S'_{os} \equiv \bm S_{os} \bm \sigma_{ss}^{-1/2}
\end{eqnarray}
%
It is then straightforward to show that the variance-covariance matrix associated to $\bm S'_{os}$ is a unit matrix:
%
\begin{eqnarray}
(\bm S')^t \bm S' = 
%
\left( \bm \sigma_{ss}^{-1/2} \bm S_{so} \right) \left( \bm S_{os} \bm \sigma_{ss}^{-1/2} \right) =
%
\bm \sigma_{ss}^{-1/2} \bm \sigma_{ss} \bm \sigma_{ss}^{-1/2} = {\bm 1}_{ss}.
\end{eqnarray}


\bibliographystyle{plain} 
\bibliography{openpca}

\end{document}  